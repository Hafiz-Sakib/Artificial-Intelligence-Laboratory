# -*- coding: utf-8 -*-
"""cassava.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oqvq17dG7h0A_pPng73fWQMxdkinSVGd

# **Import Libraries**
"""

# Step 1: Import necessary libraries
import pandas as pd
import os
from google.colab import drive
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import Callback, ModelCheckpoint
from tensorflow.keras.models import load_model
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

"""# **Mount Google Drive and Define Paths and Create Folders**"""

# Step 2: Mount Google Drive to access files
drive.mount('/content/drive')

# Step 3: Define paths for input and output folders
input_folder = "/content/drive/MyDrive/Dataset/train/train_data"  # Folder containing original images
csv_path = "/content/drive/MyDrive/Dataset/train/other/train.csv"  # Path to the CSV file
output_folder = "/content/drive/MyDrive/Dataset/train/augmented_images"  # Folder to save augmented images

# Create the output folder if it doesn't exist
os.makedirs(output_folder, exist_ok=True)
print(f"Output folder created at: {output_folder}")

"""# **Load CSV File and Add Class Names**"""

# Step 5: Load the CSV file containing image IDs and labels
df = pd.read_csv(csv_path)
print("CSV file loaded successfully. First few rows:")
print(df.head())

# Step 6: Define a mapping for labels to class names
label_mapping = {
    0: "Cassava Bacterial Blight (CBB)",
    1: "Cassava Brown Streak Disease (CBSD)",
    2: "Cassava Green Mottle (CGM)",
    3: "Cassava Mosaic Disease (CMD)",
    4: "Healthy"
}

# Step 7: Add a new column to the DataFrame for class names
df['class_name'] = df['label'].map(label_mapping)
print("DataFrame with class names added:")
print(df.head())

"""# **Split Dataset into Training and Validation Sets**"""

# Step 8: Split the dataset into training and validation sets
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)
print("Training samples:", len(train_df))
print("Validation samples:", len(val_df))

"""# **Define Image Dimensions and Data Augmentation**"""

# Step 9: Define image dimensions and batch size
img_height, img_width = 150, 150  # Resize images to 150x150 pixels
batch_size = 32  # Number of images processed in each batch

# Step 10: Set up data augmentation for training data
train_datagen = ImageDataGenerator(
    rescale=1./255,  # Normalize pixel values to [0, 1]
    rotation_range=20,  # Randomly rotate images by 20 degrees
    width_shift_range=0.2,  # Randomly shift images horizontally by 20%
    height_shift_range=0.2,  # Randomly shift images vertically by 20%
    shear_range=0.2,  # Apply shear transformations
    zoom_range=0.2,  # Randomly zoom images by 20%
    horizontal_flip=True,  # Randomly flip images horizontally
    fill_mode='nearest'  # Fill missing pixels with the nearest value
)

# Step 11: Only rescale validation data (no augmentation)
val_datagen = ImageDataGenerator(rescale=1./255)

"""# **Create Data Generators**"""

# Step 12: Create a training data generator
train_generator = train_datagen.flow_from_dataframe(
    dataframe=train_df,  # Training DataFrame
    directory=input_folder,  # Folder containing images
    x_col='image_id',  # Column with image filenames
    y_col='label',  # Column with integer labels
    target_size=(img_height, img_width),  # Resize images
    batch_size=batch_size,  # Batch size
    class_mode='raw'  # Use 'raw' for integer labels
)

# Step 13: Create a validation data generator
val_generator = val_datagen.flow_from_dataframe(
    dataframe=val_df,  # Validation DataFrame
    directory=input_folder,  # Folder containing images
    x_col='image_id',  # Column with image filenames
    y_col='label',  # Column with integer labels
    target_size=(img_height, img_width),  # Resize images
    batch_size=batch_size,  # Batch size
    class_mode='raw',  # Use 'raw' for integer labels
    shuffle=False  # Do not shuffle validation data
)

"""# **Visualize and Save Augmented Images**"""

# Step 14: Function to visualize and save 8 types of augmented images in separate folders
def visualize_and_save_augmented_images(generator, save_dir, num_images=30):
    # Get the first batch of images and labels
    images, labels = next(generator)

    # Get the original filenames from the generator
    filenames = generator.filenames

    # Create a subfolder for each image and save the augmented images
    for i in range(num_images):
        # Extract the original image's filename (without extension) and label
        original_filename = os.path.splitext(os.path.basename(filenames[i]))[0]  # Get filename without extension
        label = labels[i]  # Get the label

        # Create a folder name using the original filename and label
        folder_name = f"{original_filename}_label_{label}"
        image_folder = os.path.join(save_dir, folder_name)  # Create folder for each image
        os.makedirs(image_folder, exist_ok=True)

        # Save the original image
        img = array_to_img(images[i])  # Convert array to image
        img.save(os.path.join(image_folder, 'original_image.jpg'))  # Save original image

        # Display the original image
        plt.figure()
        plt.imshow(images[i])
        plt.title(f'Original Image - Label: {label}')
        plt.axis('off')
        plt.show()

        # Define augmentation parameters explicitly
        augmentations = [
            {'rescale': 1./255, 'filename': 'rescale=1.0_255.jpg'},
            {'rotation_range': 20, 'filename': 'rotation_range=20.jpg'},
            {'width_shift_range': 0.2, 'filename': 'width_shift_range=0.2.jpg'},
            {'height_shift_range': 0.2, 'filename': 'height_shift_range=0.2.jpg'},
            {'shear_range': 0.2, 'filename': 'shear_range=0.2.jpg'},
            {'zoom_range': 0.2, 'filename': 'zoom_range=0.2.jpg'},
            {'horizontal_flip': True, 'filename': 'horizontal_flip=True.jpg'},
            {'fill_mode': 'nearest', 'filename': 'fill_mode=nearest.jpg'}
        ]

        # Apply each augmentation and save the image
        for aug in augmentations:
            # Create a new ImageDataGenerator with the specific augmentation
            datagen = ImageDataGenerator(**{k: v for k, v in aug.items() if k != 'filename'})
            augmented_image = datagen.random_transform(images[i])  # Apply the augmentation
            augmented_img = array_to_img(augmented_image)  # Convert array to image
            augmented_img.save(os.path.join(image_folder, aug['filename']))  # Save augmented image

            # Display the augmented image
            plt.figure()
            plt.imshow(augmented_image)
            plt.title(f'Augmented Image: {aug["filename"]} - Label: {label}')
            plt.axis('off')
            plt.show()

# Step 15: Call the function to visualize and save augmented images
visualize_and_save_augmented_images(train_generator, output_folder)

"""# **Define and Compile the Model**"""

# Step 16: Define the CNN model architecture
num_classes = len(label_mapping)  # Number of unique classes
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),  # First convolutional layer
    layers.MaxPooling2D((2, 2)),  # Max pooling layer
    layers.Conv2D(64, (3, 3), activation='relu'),  # Second convolutional layer
    layers.MaxPooling2D((2, 2)),  # Max pooling layer
    layers.Conv2D(128, (3, 3), activation='relu'),  # Third convolutional layer
    layers.MaxPooling2D((2, 2)),  # Max pooling layer
    layers.Flatten(),  # Flatten the output
    layers.Dense(512, activation='relu'),  # Fully connected layer
    layers.Dropout(0.5),  # Dropout layer to prevent overfitting
    layers.Dense(num_classes, activation='softmax')  # Output layer with softmax activation
])

# Step 17: Compile the model
model.compile(
    optimizer='adam',  # Optimizer
    loss='sparse_categorical_crossentropy',  # Loss function for integer labels
    metrics=['accuracy']  # Metric to monitor
)

# Step 18: Print the model summary
print("Model Summary:")
model.summary()

"""# **Define Callbacks and Train the Model**"""

# Step 19: Define callbacks for training
epochs = 20  # Number of training epochs

# Custom callback to print accuracy after each epoch
class SaveAndPrintAccuracy(Callback):
    def on_epoch_end(self, epoch, logs=None):
        accuracy = logs.get('accuracy')
        val_accuracy = logs.get('val_accuracy')
        print(f"\nEpoch {epoch + 1}: Training Accuracy = {accuracy:.4f}, Validation Accuracy = {val_accuracy:.4f}")

# ModelCheckpoint callback to save the best model
checkpoint_callback = ModelCheckpoint(
    filepath=os.path.join(models_folder, 'model_epoch_{epoch:02d}_val_acc_{val_accuracy:.4f}.keras'),
    save_freq='epoch',  # Save after each epoch
    save_weights_only=False,  # Save the entire model
    verbose=1  # Print messages
)

# Step 20: Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,  # Number of steps per epoch
    validation_data=val_generator,
    validation_steps=val_generator.samples // batch_size,  # Number of validation steps
    epochs=epochs,  # Number of epochs
    callbacks=[checkpoint_callback, SaveAndPrintAccuracy()]  # Callbacks
)

"""# **Save and Load the Final Model**"""

# Step 21: Save the final trained model
final_model_path = os.path.join(models_folder, 'disease_classifier.h5')
model.save(final_model_path)
print(f"Final model saved at: {final_model_path}")

# Step 22: Load the saved model
model = load_model(final_model_path)
print("Model loaded successfully.")

"""# **Prediction Function and Example Usage**"""

# Step 23: Function to predict the class of a new image
def predict_image(image_path):
    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(img_height, img_width))  # Load image
    img_array = tf.keras.preprocessing.image.img_to_array(img)  # Convert image to array
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array /= 255.0  # Normalize pixel values
    predictions = model.predict(img_array)  # Make predictions
    predicted_class = np.argmax(predictions, axis=1)  # Get the predicted class
    predicted_label = label_mapping[predicted_class[0]]  # Map class index to label
    return predicted_label

# Step 24: Example usage of the prediction function
test_image_path = os.path.join(input_folder, 'example_image.jpg')  # Replace with an actual image path
if os.path.exists(test_image_path):
    print(f'Predicted class: {predict_image(test_image_path)}')
else:
    print(f"Test image not found at {test_image_path}")