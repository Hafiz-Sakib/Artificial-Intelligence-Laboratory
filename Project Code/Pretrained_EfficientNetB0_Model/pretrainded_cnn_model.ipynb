{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAoQozqB2-9X"
      },
      "source": [
        "# **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OF7d_Pdx3CR-"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0qP28lD3ECZ"
      },
      "source": [
        "# **Mount Google Drive and Define Paths and Create Folders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dEh0GA493MFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "544ff29d-f990-450b-f562-22b292b1f0ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Define Paths\n",
        "input_folder = \"/content/drive/MyDrive/Dataset/train/train_data\"\n",
        "csv_path = \"/content/drive/MyDrive/Dataset/train/other/train.csv\"\n",
        "output_folder = \"/content/drive/MyDrive/Dataset/train/EfficientNetB0/augmented_img\"\n",
        "models_folder = \"/content/drive/MyDrive/Dataset/train/EfficientNetB0/models\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkXsT8wSOgiT",
        "outputId": "c1f1d9dc-53f0-4984-a40b-b1764acc7e50"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CaP8XEu3luF"
      },
      "source": [
        "# **Load CSV File and Add Class Names**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_xBMR0Nj3nUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "392c82b9-7226-455c-d7ea-89bc8a9bf27d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded and class names mapped successfully!\n",
            "First 5 rows of the dataset:\n",
            "         image_id  label                           class_name\n",
            "0  1000015157.jpg      0       Cassava Bacterial Blight (CBB)\n",
            "1  1000201771.jpg      3         Cassava Mosaic Disease (CMD)\n",
            "2   100042118.jpg      1  Cassava Brown Streak Disease (CBSD)\n",
            "3  1000723321.jpg      1  Cassava Brown Streak Disease (CBSD)\n",
            "4  1000812911.jpg      3         Cassava Mosaic Disease (CMD)\n"
          ]
        }
      ],
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Define label mapping\n",
        "label_mapping = {\n",
        "    0: \"Cassava Bacterial Blight (CBB)\",\n",
        "    1: \"Cassava Brown Streak Disease (CBSD)\",\n",
        "    2: \"Cassava Green Mottle (CGM)\",\n",
        "    3: \"Cassava Mosaic Disease (CMD)\",\n",
        "    4: \"Healthy\"\n",
        "}\n",
        "\n",
        "# Map labels to class names\n",
        "df['class_name'] = df['label'].map(label_mapping)\n",
        "\n",
        "# Show a message confirming the full execution\n",
        "print(\"Dataset loaded and class names mapped successfully!\")\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP4CLH8Y3pT_"
      },
      "source": [
        "# **Split Dataset into Training and Validation Sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZdLnnSO3sy6",
        "outputId": "512fc94e-9ae7-4b8e-8160-4502ac824076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split into training and validation sets: 17117 training samples and 4280 validation samples.\n"
          ]
        }
      ],
      "source": [
        "# Train-Validation Split\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print a message confirming the split\n",
        "print(f\"Dataset split into training and validation sets: {len(train_df)} training samples and {len(val_df)} validation samples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYn-SjLg3vBz"
      },
      "source": [
        "# **Define Image Dimensions and Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VU39Xw8c3zBJ"
      },
      "outputs": [],
      "source": [
        "# Image Parameters\n",
        "img_height, img_width = 224, 224\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k0i1Qtj3zxg"
      },
      "source": [
        "# **Create Data Generators**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iKu6xhT133iu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ea236d-e4ca-4c0e-fc47-b2162fc9e446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 13484 validated image filenames.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py:920: UserWarning: Found 3633 invalid image filename(s) in x_col=\"image_id\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3414 validated image filenames.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py:920: UserWarning: Found 866 invalid image filename(s) in x_col=\"image_id\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "# Data Generators\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory=input_folder,\n",
        "    x_col='image_id',\n",
        "    y_col='label',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='raw'\n",
        ")\n",
        "val_generator = val_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    directory=input_folder,\n",
        "    x_col='image_id',\n",
        "    y_col='label',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='raw',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS6CjfP834cm"
      },
      "source": [
        "# **Visualize and Save Augmented Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Imz1Gwf38GH"
      },
      "outputs": [],
      "source": [
        "# Function to visualize and save augmented images with corresponding augmentation types in filenames\n",
        "def visualize_and_save_augmented_images(generator, save_dir, num_images=30):\n",
        "    # Get the first batch of images and labels\n",
        "    images, labels = next(generator)\n",
        "\n",
        "    # Get the original filenames from the generator\n",
        "    filenames = generator.filenames\n",
        "\n",
        "    # Define augmentation types and their filenames\n",
        "    augmentations = [\n",
        "        {'rescale': 1./255, 'filename': 'rescale_1.0_255.jpg'},\n",
        "        {'rotation_range': 40, 'filename': 'rotation_range_40.jpg'},\n",
        "        {'width_shift_range': 0.3, 'filename': 'width_shift_range_0.3.jpg'},\n",
        "        {'height_shift_range': 0.3, 'filename': 'height_shift_range_0.3.jpg'},\n",
        "        {'shear_range': 0.3, 'filename': 'shear_range_0.3.jpg'},\n",
        "        {'zoom_range': 0.3, 'filename': 'zoom_range_0.3.jpg'},\n",
        "        {'horizontal_flip': True, 'filename': 'horizontal_flip_True.jpg'},\n",
        "        {'vertical_flip': True, 'filename': 'vertical_flip_True.jpg'},\n",
        "        {'brightness_range': [0.8, 1.2], 'filename': 'brightness_range_0.8_1.2.jpg'},\n",
        "        {'fill_mode': 'nearest', 'filename': 'fill_mode_nearest.jpg'}\n",
        "    ]\n",
        "\n",
        "    # Create a subfolder for each image and save the augmented images\n",
        "    for i in range(num_images):\n",
        "        # Extract the original image's filename (without extension) and label\n",
        "        original_filename = os.path.splitext(os.path.basename(filenames[i]))[0]  # Get filename without extension\n",
        "        label = labels[i]  # Get the label\n",
        "\n",
        "        # Create a folder name using the original filename and label\n",
        "        folder_name = f\"{original_filename}_label_{label}\"\n",
        "        image_folder = os.path.join(save_dir, folder_name)  # Create folder for each image\n",
        "        os.makedirs(image_folder, exist_ok=True)\n",
        "\n",
        "        # Save the original image\n",
        "        img = array_to_img(images[i])  # Convert array to image\n",
        "        img.save(os.path.join(image_folder, 'original_image.jpg'))  # Save original image\n",
        "\n",
        "        # Display the original image\n",
        "        plt.figure()\n",
        "        plt.imshow(images[i])\n",
        "        plt.title(f'Original Image - Label: {label}')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Apply each augmentation and save the image with corresponding filename\n",
        "        for aug in augmentations:\n",
        "            # Create a new ImageDataGenerator with the specific augmentation\n",
        "            datagen = ImageDataGenerator(**{k: v for k, v in aug.items() if k != 'filename'})\n",
        "            augmented_image = datagen.random_transform(images[i])  # Apply the augmentation\n",
        "            augmented_img = array_to_img(augmented_image)  # Convert array to image\n",
        "\n",
        "            # Save augmented image with the augmentation type in the filename\n",
        "            augmented_filename = aug['filename']\n",
        "            augmented_img.save(os.path.join(image_folder, augmented_filename))  # Save augmented image\n",
        "\n",
        "            # Display the augmented image\n",
        "            plt.figure()\n",
        "            plt.imshow(augmented_image)\n",
        "            plt.title(f'Augmented Image: {augmented_filename} - Label: {label}')\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "# Call the function to visualize and save augmented images\n",
        "visualize_and_save_augmented_images(train_generator, output_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm57tKwI3_zT"
      },
      "source": [
        "# **Define and Compile the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INHDdgJw4CYP"
      },
      "outputs": [],
      "source": [
        "# Step 16: Define the CNN model architecture\n",
        "num_classes = len(label_mapping)  # Number of unique classes\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),  # First convolutional layer\n",
        "    layers.MaxPooling2D((2, 2)),  # Max pooling layer\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),  # Second convolutional layer\n",
        "    layers.MaxPooling2D((2, 2)),  # Max pooling layer\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),  # Third convolutional layer\n",
        "    layers.MaxPooling2D((2, 2)),  # Max pooling layer\n",
        "    layers.Flatten(),  # Flatten the output\n",
        "    layers.Dense(512, activation='relu'),  # Fully connected layer\n",
        "    layers.Dropout(0.5),  # Dropout layer to prevent overfitting\n",
        "    layers.Dense(num_classes, activation='softmax')  # Output layer with softmax activation\n",
        "])\n",
        "\n",
        "# Step 17: Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',  # Optimizer\n",
        "    loss='sparse_categorical_crossentropy',  # Loss function for integer labels\n",
        "    metrics=['accuracy']  # Metric to monitor\n",
        ")\n",
        "\n",
        "# Step 18: Print the model summary\n",
        "print(\"Model Summary:\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44vqi2mU4Ecr"
      },
      "source": [
        "# **Define Callbacks and Train the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHvJA-1d4IDs"
      },
      "outputs": [],
      "source": [
        "# Call function to visualize and save augmented images\n",
        "visualize_and_save_augmented_images(train_generator, output_folder)\n",
        "\n",
        "# Compute Class Weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(df['label']), y=df['label'])\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Define Model\n",
        "base_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(img_height, img_width, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(len(label_mapping), activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb6zvwFa4KmE"
      },
      "source": [
        "# **Compile Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0dzmyna4L3T"
      },
      "outputs": [],
      "source": [
        "# Compile Model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Callbacks**"
      ],
      "metadata": {
        "id": "5puJBie7IH0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Callbacks\n",
        "class CustomModelCheckpoint(ModelCheckpoint):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        super().on_epoch_end(epoch, logs)\n",
        "        print(f\"Model saved successfully at epoch {epoch + 1} with validation accuracy: {logs.get('val_accuracy'):.4f}\")\n",
        "\n",
        "checkpoint_callback = CustomModelCheckpoint(\n",
        "    filepath=os.path.join(models_folder, 'model_epoch_{epoch:02d}_val_acc_{val_accuracy:.4f}.keras'),\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "mr3PYKQJIKjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train and save The Final Model**"
      ],
      "metadata": {
        "id": "UtmocaVNIRvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model\n",
        "epochs = 50\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=epochs,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[checkpoint_callback, lr_scheduler, early_stopping]\n",
        ")\n",
        "# Save Final Model\n",
        "final_model_path = os.path.join(models_folder, 'final_disease_classifier.h5')\n",
        "model.save(final_model_path)\n",
        "print(f\"Final model saved at: {final_model_path}\")"
      ],
      "metadata": {
        "id": "oD7X52E7IQuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTJ1rxJO4O6i"
      },
      "source": [
        "# **Prediction Function and Example Usage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8-HmQG84SCj"
      },
      "outputs": [],
      "source": [
        "# Test Data Directory\n",
        "test_folder = \"/content/drive/MyDrive/Dataset/train/test_data\"  # Adjust the path to your test data folder\n",
        "\n",
        "# Test Data Generator\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Test Generator (adjust according to your test dataset)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=test_folder,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=None,  # We don't have labels in the test set\n",
        "    shuffle=False  # Important: We don't shuffle the test data\n",
        ")\n",
        "\n",
        "# Prediction Function for Batch Prediction\n",
        "def predict_test_data(generator):\n",
        "    filenames = generator.filenames\n",
        "    predictions = model.predict(generator, verbose=1)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)  # Get the predicted class indices\n",
        "    predicted_labels = [label_mapping[class_idx] for class_idx in predicted_classes]  # Map class index to label\n",
        "\n",
        "    # Display predictions and save images\n",
        "    for i, (filename, label) in enumerate(zip(filenames, predicted_labels)):\n",
        "        image_path = os.path.join(test_folder, filename)\n",
        "        img = tf.keras.preprocessing.image.load_img(image_path)  # Load the image\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"Predicted: {label}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    return filenames, predicted_labels\n",
        "\n",
        "# Call the function to predict and visualize test images\n",
        "test_filenames, predicted_labels = predict_test_data(test_generator)\n",
        "\n",
        "# Print the predicted labels and class names\n",
        "for filename, label in zip(test_filenames, predicted_labels):\n",
        "    print(f\"Image: {filename} - Predicted Label: {label} - Class Name: {label_mapping[int(label)]}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}